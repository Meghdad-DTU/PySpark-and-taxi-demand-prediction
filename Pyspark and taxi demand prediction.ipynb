{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/home/meghdad/spark-2.4.5-bin-hadoop2.7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"NYC_taxi_demnad\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.csv(\"pickups+weather_wallstreet.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65712 15\n",
      "root\n",
      " |-- datetime: string (nullable = true)\n",
      " |-- pickups: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- min_temp: double (nullable = true)\n",
      " |-- max_temp: double (nullable = true)\n",
      " |-- wind_speed: double (nullable = true)\n",
      " |-- wind_gust: double (nullable = true)\n",
      " |-- visibility: double (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      " |-- precipitation: double (nullable = true)\n",
      " |-- snow_depth: double (nullable = true)\n",
      " |-- fog: integer (nullable = true)\n",
      " |-- rain_drizzle: integer (nullable = true)\n",
      " |-- snow_ice: integer (nullable = true)\n",
      " |-- thunder: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## number of rows and columns\n",
    "print(df.count(),len(df.columns))\n",
    "\n",
    "## variable types\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009-01-01 00\n",
      "47\n",
      "2009-01-01 00:00:00\n",
      "15.1\n",
      "26.1\n",
      "11.6\n",
      "32.1\n",
      "10.0\n",
      "1015.5\n",
      "0.04\n",
      "0.0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "## an example of what each row includes:\n",
    "for item in df.head(1)[0]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----+--------+--------+----------+---------+----------+--------+-------------+----------+---+------------+--------+-------+\n",
      "|datetime|pickups|date|min_temp|max_temp|wind_speed|wind_gust|visibility|pressure|precipitation|snow_depth|fog|rain_drizzle|snow_ice|thunder|\n",
      "+--------+-------+----+--------+--------+----------+---------+----------+--------+-------------+----------+---+------------+--------+-------+\n",
      "|       0|      0|   0|       0|       0|         0|        0|         0|       0|            0|         0|  0|           0|       0|      0|\n",
      "+--------+-------+----+--------+--------+----------+---------+----------+--------+-------------+----------+---+------------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## missing values in dataframe\n",
    "from pyspark.sql.functions import col,sum\n",
    "df.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datetime', 'pickups', 'date', 'min_temp', 'max_temp', 'wind_speed', 'wind_gust', 'visibility', 'pressure', 'precipitation', 'snow_depth', 'fog', 'rain_drizzle', 'snow_ice', 'thunder', 'tod', 'dow']\n"
     ]
    }
   ],
   "source": [
    "## adding two columns of day of week(dow) and time of day(tod)\n",
    "from pyspark.sql.functions import split,date_format,col\n",
    "# df.select(df.columns[1:],date_format(\"date\",\"E\").alias(\"dow\"))\n",
    "\n",
    "split_column=split(df[\"datetime\"],\"-\")\n",
    "df=df.withColumn(\"tod\",split_column.getItem(1))\n",
    "df=df.withColumn(\"dow\",date_format(\"date\",\"E\"))\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datetime', 'pickups', 'date', 'min_temp', 'max_temp', 'wind_speed', 'wind_gust', 'visibility', 'pressure', 'precipitation', 'snow_depth', 'fog', 'rain_drizzle', 'snow_ice', 'thunder', 'tod', 'dow', 'Sun', 'Mon', 'Thu', 'Sat', 'Wed', 'Fri', 'Tue', '07', '11', '01', '09', '05', '08', '03', '02', '06', '10', '12', '04']\n"
     ]
    }
   ],
   "source": [
    "## getting dummy variables for dow and tod\n",
    "from pyspark.sql.functions import when,col\n",
    "# list of ounique items in \"dow\" column\n",
    "dow_cats=df.select(\"dow\").distinct().rdd.flatMap(lambda x:x).collect()\n",
    "exprs_dow=[when(col(\"dow\")==cat,1).otherwise(0).alias(str(cat)) for cat in dow_cats]\n",
    "\n",
    "tod_cats=df.select(\"tod\").distinct().rdd.flatMap(lambda x:x).collect()\n",
    "exprs_tod=[when(col(\"tod\")==cat,1).otherwise(0).alias(str(cat)) for cat in tod_cats]\n",
    "\n",
    "\n",
    "\n",
    "df=df.select(df.columns+exprs_dow+exprs_tod)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+-------+\n",
      "|               date|Lagged_pickups|pickups|\n",
      "+-------------------+--------------+-------+\n",
      "|2009-01-01 00:00:00|          null|     47|\n",
      "|2009-01-01 00:00:00|            47|     74|\n",
      "|2009-01-01 00:00:00|            74|     79|\n",
      "|2009-01-01 00:00:00|            79|     57|\n",
      "|2009-01-01 00:00:00|            57|     46|\n",
      "|2009-01-01 00:00:00|            46|     18|\n",
      "|2009-01-01 00:00:00|            18|     16|\n",
      "|2009-01-01 00:00:00|            16|      7|\n",
      "|2009-01-01 00:00:00|             7|     10|\n",
      "|2009-01-01 00:00:00|            10|     10|\n",
      "+-------------------+--------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## adding lagged pickups to the dataframe\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag\n",
    "my_window=Window.partitionBy().orderBy(\"date\")\n",
    "df=df.withColumn(\"Lagged_pickups\",lag(df[\"pickups\"]).over(my_window))\n",
    "df.select(\"date\",\"Lagged_pickups\",\"pickups\").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting irrevant columns\n",
    "columns_to_drop=['datetime',\"date\",'dow',\"tod\"]\n",
    "df=df.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression model without lagged variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(31,[0,1,2,3,4,5,...|\n",
      "|(31,[0,1,2,3,4,5,...|\n",
      "|(31,[0,1,2,3,4,5,...|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_variables=df.columns[1:-1]\n",
    "# It grabs X columnes and converts them into a single column of feature\n",
    "assembler=VectorAssembler(inputCols=X_variables,outputCol=\"features\")\n",
    "output= assembler.transform(df)\n",
    "output.select(\"features\").show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|pickups|            features|\n",
      "+-------+--------------------+\n",
      "|     47|(31,[0,1,2,3,4,5,...|\n",
      "|     74|(31,[0,1,2,3,4,5,...|\n",
      "|     79|(31,[0,1,2,3,4,5,...|\n",
      "|     57|(31,[0,1,2,3,4,5,...|\n",
      "+-------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## determine final data including X:features and y:Crew\n",
    "final_data=output.select(\"pickups\",\"features\")\n",
    "final_data.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|          pickups|\n",
      "+-------+-----------------+\n",
      "|  count|            45978|\n",
      "|   mean|83.26021140545478|\n",
      "| stddev|48.81862166103399|\n",
      "|    min|                0|\n",
      "|    max|              302|\n",
      "+-------+-----------------+\n",
      "\n",
      "None\n",
      "+-------+-----------------+\n",
      "|summary|          pickups|\n",
      "+-------+-----------------+\n",
      "|  count|            19734|\n",
      "|   mean|83.03349549001723|\n",
      "| stddev|48.97198227418427|\n",
      "|    min|                0|\n",
      "|    max|              273|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## split data into train and test set\n",
    "train_data,test_data=final_data.randomSplit([0.7,0.3])\n",
    "\n",
    "print(train_data.describe().show())\n",
    "test_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "## linear regression model\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr=LinearRegression(featuresCol=\"features\",\n",
    "                    labelCol=\"pickups\",\n",
    "                    predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fit linear model\n",
    "lr_model=lr.fit(train_data)\n",
    "\n",
    "## predict model\n",
    "test_results=lr_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error is: 47.559\n",
      "Mean Absolute Error is: 38.912\n",
      "R squared is: 0.057\n"
     ]
    }
   ],
   "source": [
    "## Regression evaluation matrix\n",
    "print(\"Root Mean Squared Error is:\",round(test_results.rootMeanSquaredError,3))\n",
    "print(\"Mean Absolute Error is:\",round(test_results.meanAbsoluteError,3))\n",
    "print(\"R squared is:\",round(test_results.r2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression model with lagged variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(32,[0,1,2,3,4,5,...|\n",
      "|(32,[0,1,2,3,4,5,...|\n",
      "|(32,[0,1,2,3,4,5,...|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.dropna()\n",
    "X_variables=df.columns[1:]\n",
    "# It grabs X columnes and converts them into a single column of feature\n",
    "assembler=VectorAssembler(inputCols=X_variables,outputCol=\"features\")\n",
    "output= assembler.transform(df)\n",
    "output.select(\"features\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "## determine final data including X:features and y:Crew\n",
    "final_data=output.select(\"pickups\",\"features\")\n",
    "\n",
    "## split data into train and test set\n",
    "train_data,test_data=final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "## linear regression model\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr=LinearRegression(featuresCol=\"features\",\n",
    "                    labelCol=\"pickups\",\n",
    "                    predictionCol=\"prediction\")\n",
    "\n",
    "## fit linear model\n",
    "lr_model=lr.fit(train_data)\n",
    "\n",
    "## predict model\n",
    "test_results=lr_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error is: 25.966\n",
      "Mean Absolute Error is: 20.492\n",
      "R squared is: 0.715\n"
     ]
    }
   ],
   "source": [
    "## Regression evaluation matrix\n",
    "print(\"Root Mean Squared Error is:\",round(test_results.rootMeanSquaredError,3))\n",
    "print(\"Mean Absolute Error is:\",round(test_results.meanAbsoluteError,3))\n",
    "print(\"R squared is:\",round(test_results.r2,3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
